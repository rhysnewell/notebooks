{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca330dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 09:27:30.604525: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-20 09:27:30.604572: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"48\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"48\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"48\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"48\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"48\" # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "\n",
    "from umap import UMAP\n",
    "import umap.plot\n",
    "\n",
    "import flowkit as fk\n",
    "import FlowCal as fc\n",
    "import seaborn as sns\n",
    "import bokeh\n",
    "from bokeh.plotting import show\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skbio\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import joypy\n",
    "\n",
    "import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "from numba import njit, set_num_threads\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "set_num_threads(48)\n",
    "\n",
    "bokeh.io.output_notebook()\n",
    "%matplotlib inline\n",
    "\n",
    "_ = plt.ioff()\n",
    "\n",
    "main_dir = \"/home/n10853499/01-projects/00-allison_microscope/\" # change this to be the top folder where the flow cyto data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2dbc138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reference_cultures(files = [main_dir + 'reference_cultures/Pro_and_syn/fixed/PFA_40_Prochlorococcus_MIT9312.fcs', main_dir + \"reference_cultures/Pro_and_syn/fixed/PFA_15_Synechococcus_PH4_0_D8.fcs\"]):\n",
    "    logicle_xform = fk.transforms.AsinhTransform(\"asinh\", param_t=262144, param_m=4.0, param_a=0.0)    \n",
    "\n",
    "    # gathering the training data\n",
    "    main_df = []\n",
    "    labels = []\n",
    "    \n",
    "    strains = {}\n",
    "    strain_index = 1\n",
    "    \n",
    "    species_map = {}\n",
    "    species_index = 1\n",
    "    \n",
    "    exps = {0: 0, 5: 1, 15: 2, 40: 3}\n",
    "    exp_index = 0\n",
    "    \n",
    "    for file in files:\n",
    "        name = file.split(\"/\")[-1].strip(\".fcs\")\n",
    "        light_exp = 0\n",
    "        strain = \"\"\n",
    "        species = \"\"\n",
    "        fixed = False\n",
    "        dilution = 0\n",
    "\n",
    "         # skip non-slow speeds\n",
    "        if any(speed in name for speed in [\"high\", \"med\"]):\n",
    "            continue\n",
    "\n",
    "        # get dilution level\n",
    "        if \"diluted\" in name:\n",
    "            if \"diluted heaps\" in name or \"diluted heap\" in name:\n",
    "                dilution = 2\n",
    "                name = name.strip(\" diluted heaps\")\n",
    "                name = name.strip(\" diluted heap\")\n",
    "            else:\n",
    "                dilution = 1\n",
    "                name = name.strip(\"_diluted\")\n",
    "                name = name.strip(\" diluted\")\n",
    "                    \n",
    "        name = name.split(\"_\")\n",
    "        print(name)\n",
    "        if \"CAS85pct\" in name:\n",
    "            species = \"Picoeukaryote\"\n",
    "            strain = \"_\".join(name[1:])\n",
    "        # get light exposure and strain name\n",
    "        elif \"LightExp\" in name or \"PFA\" in name:\n",
    "            if \"PFA\" in name:\n",
    "                fixed = True\n",
    "\n",
    "            light_exp = int(name[1])\n",
    "            # rename badly named files\n",
    "            if name[2] == \"Prochloroccocus\" or name[2] == \"Prochloroccocus\":\n",
    "                name[2] = \"Prochlorococcus\"\n",
    "            elif name[2] == \"Synecoccocus\" or name[2] == \"Synechoccocus\":\n",
    "                name[2] = \"Synechococcus\"\n",
    "            if \"MARCIA\" in name:\n",
    "                name[0] = \"Synechococcus_MARCIA\"\n",
    "            strain = \"_\".join(name[2:])\n",
    "            species = name[2]\n",
    "        else:\n",
    "            # rename badly named files\n",
    "            if name[0] == \"Prochloroccocus\" or name[0] == \"Prochloroccocus\":\n",
    "                name[0] = \"Prochlorococcus\"\n",
    "            elif name[0] == \"Synecoccocus\" or name[0] == \"Synechoccocus\":\n",
    "                name[0] = \"Synechococcus\"\n",
    "            if \"MARCIA\" in name:\n",
    "                name[0] = \"Synechococcus_MARCIA\"\n",
    "\n",
    "            name[1] = \"_\".join(name[1].split(\" \")).upper()\n",
    "            strain = \"_\".join(name)\n",
    "            species = name[0]\n",
    "        \n",
    "        print(species, strain)\n",
    "        if ((strain in [\"Prochlorococcus_CC9605\", \"Synechococcus_BBP2\", \"Ostreococcus_TAURI_RCC4221\", \"Synechococcus_CC9311\", \"Synechococcus_MARCIA_R1\"])\n",
    "            or (strain in [\"Synechococcus_WH8016\"] and fixed)\n",
    "            # or (strain in [\"Prochlorococcus_MIT9313\"] and not fixed)\n",
    "            or (strain in [\"Prochlorococcus_NATL2A\"] and light_exp == 40) \n",
    "            or (strain in [\"Prochlorococcus_SS120\"] and light_exp in [5, 15])\n",
    "            or (strain in [\"Synechococcus_MITS9220\"] and light_exp in [0] and dilution in [0])): # potentially contaminated?\n",
    "            continue\n",
    "        \n",
    "        fk_file = fk.Sample(file, subsample=50000)\n",
    "        fk_file.pns_labels = fk_file.pnn_labels\n",
    "\n",
    "        fk_file.apply_transform(logicle_xform)\n",
    "        s_g1 = fc.gate.density2d(fk_file.as_dataframe(source=\"xform\", subsample=True)[['FSC-H', 'SSC-H']].values, gate_fraction=0.75, full_output=True)\n",
    "        mask = s_g1.mask\n",
    "\n",
    "        df = fk_file.as_dataframe(source='xform', subsample=True)\n",
    "        \n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "        to_select = [(\"-A\" in col or (\"SC-\" in col and \"-W\" not in col)) for col in df.columns]\n",
    "\n",
    "        \n",
    "        print(\"Mask: \", mask.shape, mask.sum(), mask)\n",
    "        \n",
    "        df = df.loc[s_g1.mask, to_select]\n",
    "        to_filter = scatter_filter(df.loc[:, :], use_size=True, use_iqr=True)\n",
    "        print(\"To filter: \", to_filter.shape, to_filter.sum(), to_filter)\n",
    "\n",
    "        mask = ~to_filter\n",
    "        print(\"Remaining shape: \", df.loc[mask, :].shape)\n",
    "\n",
    "        df['fixed'] = fixed\n",
    "        df['dilution'] = dilution\n",
    "        # convert strain to int index\n",
    "        try:\n",
    "            df.loc[mask, 'strain'] = strains[strain]\n",
    "            df.loc[mask, 'strain_name'] = strain\n",
    "            df.loc[mask, 'species'] = species_map[species]\n",
    "        except KeyError:\n",
    "            strains[strain] = strain_index\n",
    "            strain_index += 1\n",
    "            \n",
    "            df.loc[mask, 'strain'] = strains[strain]\n",
    "            df.loc[mask, 'strain_name'] = strain\n",
    "        \n",
    "        try:\n",
    "            df.loc[mask, 'species'] = species_map[species]\n",
    "        except KeyError:\n",
    "            species_map[species] = species_index\n",
    "            species_index += 1\n",
    "            df.loc[mask, 'species'] = species_map[species]\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            df['light_exp'] = exps[light_exp]\n",
    "        except KeyError:\n",
    "            exps[light_exp] = exp_index\n",
    "            exp_index += 1\n",
    "            df['light_exp'] = exps[light_exp]\n",
    "        \n",
    "        labels.append(df.loc[mask, [\"-\" not in col for col in df.columns]])\n",
    "        main_df.append(df.loc[mask, [\"-\" in col for col in df.columns]])\n",
    "\n",
    "    main_df = pd.concat(main_df)\n",
    "\n",
    "    min_df = main_df[['FSC-A', 'SSC-A', 'SSC-B-A', 'FSC-H', 'SSC-H', 'SSC-B-H']].min()\n",
    "\n",
    "    main_df[['FSC-A', 'SSC-A', 'SSC-B-A', 'FSC-H', 'SSC-H', 'SSC-B-H']] = main_df[['FSC-A', 'SSC-A', 'SSC-B-A', 'FSC-H', 'SSC-H', 'SSC-B-H']] + abs(min_df)\n",
    "\n",
    "    main_df.reset_index(inplace=True)\n",
    "    labels = pd.concat(labels)\n",
    "    labels.reset_index(inplace=True)\n",
    "\n",
    "    subsampled_df = pd.concat([main_df, labels], axis=1)\n",
    "    subsampled_df = subsampled_df.groupby(['species']).sample(n=subsampled_df.groupby(['species']).count().iloc[:, 0].min())\n",
    "    subsampled_df.pop('index')\n",
    "    to_select = [(\"-A\" in col or (\"SC-\" in col and \"-W\" not in col)) for col in subsampled_df.columns]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df = scaler.fit_transform(subsampled_df.loc[:, to_select])\n",
    "    \n",
    "    return scaled_df, min_df, subsampled_df\n",
    "\n",
    "def scatter_filter(df, use_size=True, use_iqr=True, size_tolerance=2, iqr_tolerance=1.5):\n",
    "\n",
    "    to_filter = np.array([False for _ in range(df.shape[0])])\n",
    "    \n",
    "    if use_size:\n",
    "        min_a = df[['FSC-A', 'SSC-A']].min()\n",
    "        min_h = df[['FSC-H', 'SSC-H']].min()\n",
    "        scatter_ratio = (df[['FSC-A', 'SSC-A']].abs()).values / (df[['FSC-H', 'SSC-H']].abs() + 10e-6).values\n",
    "        print(scatter_ratio.mean(axis=0))\n",
    "        scatter_array = np.concatenate([scatter_ratio > size_tolerance, scatter_ratio < 1/size_tolerance], axis=1)\n",
    "        print(scatter_array.sum(axis=0))\n",
    "        to_filter = scatter_array.any(axis=1)\n",
    "\n",
    "    if use_iqr:\n",
    "        iqr_df = df.copy()\n",
    "        iqr_df += abs(iqr_df.min())\n",
    "        for col in iqr_df.columns:\n",
    "            Q3 = np.quantile(iqr_df[col], 0.75)\n",
    "            Q1 = np.quantile(iqr_df[col], 0.25)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # print(\"IQR value for column %s is: %s\" % (col, IQR))\n",
    "            lower_range = Q1 - iqr_tolerance * IQR\n",
    "            upper_range = Q3 + iqr_tolerance * IQR\n",
    "            to_filter += [((x < lower_range) or (x > upper_range)) for x in iqr_df[col]]\n",
    "    \n",
    "    return to_filter\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7681f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in reference culture data\n",
    "s_df, min_df, unscaled_df = reference_cultures(\n",
    "    glob.glob(main_dir + \"reference_cultures/Pro_and_syn/fixed/final/*cus*fcs\") + \n",
    "    glob.glob(main_dir + \"reference_cultures/Pro_and_syn/fresh/*cus*fcs\") + \n",
    "    glob.glob(main_dir + \"reference_cultures/Algae/final/*fcs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transect_data(paths=None, minima=min_df):\n",
    "    logicle_xform = fk.transforms.AsinhTransform(\"asinh\", param_t=262144, param_m=4.0, param_a=0.0)    \n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    main_df = []\n",
    "    unscaled_df = []\n",
    "    main_df_labels = []\n",
    "    sample_idx = 0\n",
    "    for file in paths:\n",
    "        print(f\"Processing {file}\")\n",
    "        if \"fcs\" not in file: \n",
    "            print(f\"Skipping {file}\")\n",
    "            continue\n",
    "        \n",
    "        name = file.split(\"/\")[-1].strip(\".fcs\")\n",
    "        \n",
    "        if any([check in name.lower() for check in [\"test\", \"beads\"]]): \n",
    "            print(f\"Skipping {file}\")\n",
    "            continue\n",
    "        \n",
    "        fk_file = fk.Sample(file, subsample=25000) # increase the subsample value if you want to include more cells in each transect\n",
    "        fk_file.pns_labels = fk_file.pnn_labels\n",
    "        fk_file.apply_transform(logicle_xform)\n",
    "        df = fk_file.as_dataframe(source='xform', subsample=True)\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "        to_select = [(\"-A\" in col or (\"SC-\" in col and \"-W\" not in col)) for col in df.columns]\n",
    "        \n",
    "        to_filter = scatter_filter(df, use_iqr=False, size_tolerance=3)\n",
    "        mask = to_filter\n",
    "        df = df.loc[~mask, to_select]\n",
    "        df['sample'] = name.lower().replace(\" \", \"_\")\n",
    "        df['sample_idx'] = sample_idx\n",
    "        df[['FSC-A', 'SSC-A', 'SSC-B-A', 'FSC-H', 'SSC-H', 'SSC-B-H']] = df[['FSC-A', 'SSC-A', 'SSC-B-A', 'FSC-H', 'SSC-H', 'SSC-B-H']] + abs(minima)\n",
    "        sample_idx += 1\n",
    "        unscaled_df.append(df.loc[:, [\"-\" in col for col in df.columns]])\n",
    "        scaled_df = scaler.fit_transform(df.loc[:, [\"-\" in col for col in df.columns]])\n",
    "        \n",
    "        main_df.append(scaled_df)\n",
    "        main_df_labels.append(df.loc[:, [\"-\" not in col for col in df.columns]])\n",
    "\n",
    "    main_df = np.concatenate(main_df, axis=0)\n",
    "    unscaled_df = pd.concat(unscaled_df)\n",
    "    unscaled_df.reset_index(inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    labels = pd.concat(main_df_labels)\n",
    "    labels['transect_label'] = labels['sample'].apply(lambda s: int(s.split(\"_\")[2].strip('ft')))\n",
    "    labels.reset_index(inplace=True)\n",
    "    return main_df, labels, unscaled_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_data, transect_labels, transect_unscaled = read_transect_data(\n",
    "    glob.glob(main_dir + \"/transect_data/trimmed/*.fcs\"),\n",
    "    min_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP embedding for transect data\n",
    "\n",
    "embedder_transect = umap.UMAP(n_neighbors=100, a=1.48, b=0.4)\n",
    "embedder_transect.fit(transect_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28881c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the TRUTH clusters for the UMAP data. Check that these clusters look right via the plot\n",
    "\n",
    "transect_labels['Pro'] = np.array([embedder_transect.embedding_[:, 0] < 0, embedder_transect.embedding_[:, 1] < 10]).all(axis=0)\n",
    "transect_labels['Syn'] = np.array([embedder_transect.embedding_[:, 0] > 10, embedder_transect.embedding_[:, 1] < 10]).all(axis=0)\n",
    "transect_labels['Pico'] = np.array([embedder_transect.embedding_[:, 0] > 5, embedder_transect.embedding_[:, 1] > 15]).all(axis=0)\n",
    "\n",
    "transect_labels['Truth'] = 'Debris'\n",
    "transect_labels['Truth'].iloc[transect_labels['Pro'] == True] = 'Pro'\n",
    "transect_labels['Truth'].iloc[transect_labels['Pico'] == True] = 'Pico'\n",
    "transect_labels['Truth'].iloc[transect_labels['Syn'] == True] = 'Syn'\n",
    "# pd.concat([pd.DataFrame(transect_data), transect_labels[['Truth', 'transect_label']].reset_index()], axis=1).drop(\"index\", axis=1).to_csv(\"00-jupyter/10-allison_umap/transect_labels_umap.tsv\", sep='\\t', index=False)\n",
    "print(transect_labels.groupby(['Pro', 'Syn', 'Pico', 'Debris']).count())\n",
    "umap.plot.points(embedder_transect, labels=transect_labels['Truth'], theme=\"fire\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89795b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot of above clutering\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=plot_transect,\n",
    "    x = \"YG1-A\",\n",
    "    y = \"B8-A\",\n",
    "    hue = \"Truth\",\n",
    "    alpha= 0.1\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of truth clusters\n",
    "\n",
    "transect_joy_df = pd.concat([pd.DataFrame(transect_data), transect_labels[['Truth', 'transect_label']].reset_index()], axis=1).drop(\"index\", axis=1).groupby(['Truth', 'transect_label']).mean().transpose()\n",
    "sns.heatmap(transect_joy_df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set species clusters to -1 before we do internal clustering\n",
    "\n",
    "transect_labels['cluster'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82900bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prochlorococcus embedding\n",
    "species_data = transect_data[(transect_labels[\"Truth\"]==\"Pro\").values]\n",
    "embedder_pro = umap.UMAP(n_neighbors=100, a=1.48, b=0.4)\n",
    "embedder_pro.fit(species_data)\n",
    "clusterer_pro = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=100, gen_min_span_tree=True, prediction_data=True)\n",
    "clusterer_pro.fit(embedder_pro.embedding_)\n",
    "\n",
    "transect_labels['cluster'][transect_labels[\"Truth\"] == \"Pro\"] = [f\"pro_{c}\" for c in clusterer_pro.labels_]\n",
    "umap.plot.points(embedder_pro, labels=clusterer_pro.labels_, theme=\"fire\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95500103",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_syn = umap.UMAP(n_neighbors=100, a=1.48, b=0.4)\n",
    "embedder_syn.fit(transect_data[(transect_labels[\"Truth\"]==\"Syn\").values])\n",
    "clusterer_syn = hdbscan.HDBSCAN(min_cluster_size=100, gen_min_span_tree=True)\n",
    "clusterer_syn.fit(embedder_syn.embedding_)\n",
    "\n",
    "# umap.plot.points(embedder_syn, labels=transect_labels[transect_labels['Truth']==\"Syn\"][\"transect_label\"].values, theme=\"fire\", alpha=0.5)\n",
    "umap.plot.points(embedder_syn, labels=clusterer_syn.labels_, theme=\"fire\", alpha=0.5)\n",
    "transect_labels['cluster'][transect_labels[\"Truth\"] == \"Syn\"] = [f\"syn_{c}\" for c in clusterer_syn.labels_]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_pico = umap.UMAP(n_neighbors=100, a=1.48, b=0.4)\n",
    "embedder_pico.fit(transect_data[(transect_labels[\"Truth\"]==\"Pico\").values])\n",
    "clusterer_pico = hdbscan.HDBSCAN(min_cluster_size=200, gen_min_span_tree=True)\n",
    "clusterer_pico.fit(embedder_pico.embedding_)\n",
    "\n",
    "# umap.plot.points(embedder_pico, labels=transect_labels[transect_labels['Truth']==\"Pico\"][\"transect_label\"].values, theme=\"fire\", alpha=0.1)\n",
    "umap.plot.points(embedder_pico, labels=clusterer_pico.labels_, theme=\"fire\", alpha=0.1)\n",
    "transect_labels['cluster'][transect_labels[\"Truth\"] == \"Pico\"] = [f\"pico_{c}\" for c in clusterer_pico.labels_]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_debris = umap.UMAP(n_neighbors=100, a=1.48, b=0.4)\n",
    "embedder_debris.fit(transect_data[(transect_labels[\"Truth\"]==\"Debris\").values])\n",
    "\n",
    "clusterer_debris = hdbscan.HDBSCAN(min_cluster_size=200, gen_min_span_tree=True)\n",
    "clusterer_debris.fit(embedder_debris.embedding_)\n",
    "\n",
    "# umap.plot.points(embedder_debris, labels=transect_labels[transect_labels['Truth']==\"Debris\"][\"transect_label\"].values, theme=\"fire\", alpha=0.5)\n",
    "umap.plot.points(embedder_debris, labels=clusterer_debris.labels_, theme=\"fire\", alpha=0.5)\n",
    "transect_labels['cluster'][transect_labels[\"Truth\"] == \"Debris\"] = [f\"debris_{c}\" for c in clusterer_debris.labels_]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb109cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write transect labels to file. Change directory to your current directory\n",
    "\n",
    "transect_labels.to_csv(\"00-jupyter/10-allison_umap/transect_labels_clustered.tsv\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "30295c5bec572e859485b1ffa5e89b8b3e2022ef6e3e739c1ac40f143a557caf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
